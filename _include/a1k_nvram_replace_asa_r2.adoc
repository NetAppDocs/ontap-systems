The NVRAM module consists of the NVRAM12 hardware and field-replaceable DIMMs. You can replace a failed NVRAM module or the DIMMs inside the NVRAM module. To replace a failed NVRAM module, you must remove the module from the enclosure, move the DIMMs to the replacement module, and install the replacement NVRAM module into the enclosure.

All other components in the system must be functioning properly; if not, you must contact https://support.netapp.com[NetApp Support].

You must replace the failed component with a replacement FRU component you received from your provider.

== Step 1: Shut down the impaired controller

Shut down or take over the impaired controller.


include::../_include/shutdown_most_frus.adoc[]

== Step 2: Replace the NVRAM module

To replace the NVRAM module, locate it in slot 4/5 in the enclosure and follow the specific sequence of steps.

. If you are not already grounded, properly ground yourself.
. Unplug the power cord from both PSUs.
. Rotate the cable management tray down by gently pulling the pins on the ends of the tray and rotating the tray down.
. Remove the impaired NVRAM module from the enclosure:
 .. Depress the locking cam button.

 .. Rotate the cam latch down as far as it will go.
+
 .. Remove the impaired NVRAM module from the enclosure by hooking your finger into the cam lever opening and pulling the module out of the enclosure.
+
image::../media/drw_a1k_nvram12_remove_replace_ieops-1380.svg[Remove the NVRAM12 module and DIMMs]
+
[cols="1,4"]
|===
a|
image:../media/icon_round_1.png[Callout number 1]|
Cam locking button
a|
image:../media/icon_round_2.png[Callout number 2]
a|
DIMM locking tabs
|===

. Set the NVRAM module on a stable surface.
. Remove the DIMMs, one at a time, from the impaired NVRAM module and install them in the replacement NVRAM module.
. Install the replacement NVRAM module into the enclosure:
 .. Align the module with the edges of the enclosure opening in slot 4/5.
 .. Gently slide the module into the slot all the way, and then rotate the cam latch all the way up to lock the module in place.
. Recable the PSUs.
. Rotate the cable management tray up to the closed position.


== Step 3: Replace a NVRAM DIMM

To replace NVRAM DIMMs in the NVRAM module, you must remove the NVRAM module, and then replace the target DIMM.

. If you are not already grounded, properly ground yourself.
. Unplug the power cord from both PSUs.
. Rotate the cable management tray down by gently pulling the pins on the ends of the tray and rotating the tray down.
. Remove the target NVRAM module from the enclosure.
+
image::../media/drw_a1k_nvram12_remove_replace_ieops-1380.svg[Remove the NVRAM 12 module and DIMMs]
+
[cols="1,4"]
|===
a|
image:../media/icon_round_1.png[Callout number 1]|
Cam locking button
a|
image:../media/icon_round_2.png[Callout number 2]
a|
DIMM locking tabs
|===

. Set the NVRAM module on a stable surface.
. Locate the DIMM to be replaced inside the NVRAM module.

+
NOTE: Consult the FRU map label on the side of the NVRAM module to determine the locations of DIMM slots 1 and 2.
+

. Remove the DIMM by pressing down on the DIMM locking tabs and lifting the DIMM out of the socket.

. Install the replacement DIMM by aligning the DIMM with the socket and gently pushing the DIMM into the socket until the locking tabs lock in place.
. Install the NVRAM module into the enclosure:
.. Gently slide the module into the slot until the cam latch begins to engage with the I/O cam pin, and then rotate the cam latch all the way up to lock the module in place.
. Recable the PSUs.
. Rotate the cable management tray up to the closed position.

== Step 4: Reboot the controller

After you replace the FRU, you must reboot the controller module.

. To boot ONTAP from the LOADER prompt, enter _bye_.

== Step 5: Verify controller state

You must confirm the controller state of the controllers connected to the disk pool when you boot the controller.


.Steps
. If the controller is in Maintenance mode (showing the `*>` prompt), exit Maintenance mode and go to the LOADER prompt: _halt_
. From the LOADER prompt on the controller, boot the controller and enter _y_ when prompted to override the system ID due to a system ID mismatch.
. Wait until the Waiting for giveback... message is displayed on the console of the controller with the replacement module and then, from the healthy controller, verify the system state: _storage failover show_
+
In the command output, you should see a message indicates the state of the controllers.
+

----

                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------------
<nodename>
               <nodename>-   true     Connected to <nodename>-P2-3-178.
               P2-3-178                Waiting for cluster applications to
                                       come online on the local node.
AFF-A90-NBC-P2-3-178
               <nodename>-   true     Connected to <nodename>-P2-3-177,
               P2-3-177                Partial giveback
2 entries were displayed.

----

. Give back the controller:
 .. From the healthy controller, give back the replaced controller's storage: _storage failover giveback -ofnode replacement_node_name_
+
The controller connects back its storage pool and completes booting.
+
If you are prompted to override the system ID due to a system ID mismatch, you should enter _y_.
+
NOTE: If the giveback is vetoed, you can consider overriding the vetoes.
+
For more information, see the https://docs.netapp.com/us-en/ontap/high-availability/ha_manual_giveback.html#if-giveback-is-interrupted[Manual giveback commands^] topic to override the veto.

 .. After the giveback has been completed, confirm that the HA pair is healthy and that takeover is possible: _storage failover show_

. Verify all disks are displayed: `storage disk show`
+

----

::> storage disk show
                     Usable           Disk    Container   Container
Disk                   Size Shelf Bay Type    Type        Name
---------------- ---------- ----- --- ------- ----------- ---------
1.0.0                3.49TB     0   0 SSD-NVM aggregate   pod_NVME_SSD_1
1.0.1                3.49TB     0   1 SSD-NVM aggregate   pod_NVME_SSD_1
1.0.2                3.49TB     0   2 SSD-NVM aggregate   pod_NVME_SSD_1
1.0.3                3.49TB     0   3 SSD-NVM aggregate   pod_NVME_SSD_1
1.0.4                3.49TB     0   4 SSD-NVM aggregate   pod_NVME_SSD_1

[...]
48 entries were displayed.

----


== Step 5: Return the failed part to NetApp

include::../_include/complete_rma.adoc[]