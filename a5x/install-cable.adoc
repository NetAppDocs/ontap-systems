---
permalink: a5x/install-cable.html
sidebar: sidebar
keywords: aff a5x, prepare installation
summary: After you install the rack hardware for your AFF A5X storage system, install the network cables for the controllers, and connect the cables between the controllers and storage shelves.
---
= Cable the hardware - AFF A5X
:icons: font
:imagesdir: ../media/

[.lead]
After you install the rack hardware for your AFF A5X storage system, install the network cables for the controllers, and connect the cables between the controllers and storage shelves.

.Before you begin

Contact your network administrator for information about connecting the storage system to the switches.

.About this task
* These procedures show common configurations. The specific cabling depends on the components ordered for your storage system. For comprehensive configuration and slot priority details, see link:https://hwu.netapp.com[NetApp Hardware Universe^].

* The I/O slots on AFF A5X controllers are numbered 1 through 11.
+
image::../media/drw_a5x_back_slots_labeled_ieops-xxxx.svg[Slot numbering on AFF A5X controllers]

* The cabling graphics have arrow icons showing the proper orientation (up or down) of the cable connector pull-tab when inserting a connector into a port.
+
As you insert the connector, you should feel it click into place; if you do not feel it click, remove it, turn it over and try again.
+
image::../media/drw_cable_pull_tab_direction_ieops-1699.svg[Cable pull tab orientation example]

* If cabling to an optical switch, insert the optical transceiver into the controller port before cabling to the switch port.

== Step 1: Connect the storage controllers to your network
Cable the controllers to your ONTAP cluster. This procedure differs depending on your storage system model and I/O module configuration.

NOTE: The cluster interconnect traffic and the HA traffic share the same physical ports.

[role="tabbed-block"]
====

.Switchless cluster cabling
--
Use the the Cluster/HA interconnect cable to connect port e10a to port e11a. 

.Steps

. Connect port e10a on Controller A to port e11a on Controller B.
. Connect port e10a on Controller B to port e11a on Controller A.
+
*Cluster/HA interconnect cables*
+
image::../media/oie_cable_25Gb_Ethernet_SFP28_ieops-1069.png[Cluster HA cable]

+
image::../media/drw_a5x_tnsc_cluster_cabling_ieops-xxxx.svg[Two-node switchless cluster cabling diagram]

--
.Switched cluster cabling
--
Use the 100 GbE cable to connect ports e12a to the cluster network switch.

.Steps

. Connect port e12a on Controller A to cluster network switch A. 
. Connect port e12a on Controller B to cluster network switch B.
+
*100 GbE cable*
+
image::../media/oie_cable100_gbe_qsfp28.png[100 Gb cable]
+
image::../media/drw_a5x_switched_cluster_cabling_ieops-xxxx.svg[Cable cluster connections to cluster network]


--

====

== Step 2: Cable the host network connections
Connect the Ethernet module ports to your host network. 

The following are some typical host network cabling examples. See  link:https://hwu.netapp.com[NetApp Hardware Universe^] for your specific system configuration.

.Steps
. Connect ports e9a and e9b to your Ethernet data network switch.
+
NOTE: For maximum system performance for cluster and HA traffic, do not use ports e1b and e7b ports for host network connections.  Use a separate host card to maximize performance.

+
*100 GbE cable*
+
image::../media/oie_cable_sfp_gbe_copper.png[100Gb Ethernet cable]
+
image::../media/drw_a5x_network_cabling1_ieops-xxxx.svg[Cable to 100 Gb Ethernet network]

+
. Connect your 10/25 GbE host network switches.
+
*4-ports, 10/25 GbE Host*
+
image::../media/oie_cable_sfp_gbe_copper.png[10/25 Gb cable]
+
image::../media/drw_a5x_network_cabling2_ieops-xxxx.svg[Cable to 100Gb Ethernet network]

== Step 3: Cable the management network connections
Use the 1000BASE-T RJ-45 cables to connect the management (wrench) ports on each controller to the management network switches.

*1000BASE-T RJ-45 cables*

image::../media/oie_cable_rj45.png[RJ45 cables]


image::../media/drw_a5x_management_connection_ieops-xxxx.svg[Connect to your management network]

IMPORTANT: Do not plug in the power cords yet.

== Step 4: Cable the shelf connections
The following cabling procedures show how to connect your controllers to a storage shelf. Choose one of the following cabling options that matches your setup.

For the maximum number of shelves supported for your storage system and for all of your cabling options, see link:https://hwu.netapp.com[NetApp Hardware Universe^].

.About this task

The AFF A5X storage system supports NS224 shelves with either the NSM100 or NSM100B module. The major differences between the modules are:  

** NSM100 shelf modules use built-in ports e0a and e0b.

** NSM100B shelf modules use ports e1a and e1b in slot 1.

The following cabling example shows NSM100 modules in the NS224 shelves when referring to shelf module ports.

[role="tabbed-block"]
====

.Option 1: One NS224 storage shelf
--
Connect each controller to the NSM modules on the NS224 shelf. The graphics show controller A cabling in blue and controller B cabling in yellow.

*100 GbE QSFP28 copper cables*

image::../media/oie_cable100_gbe_qsfp28.png[100 GbE QSFP28 copper cable]

.Steps
. Connect controller A port e11a to NSM A port e0a.
. Connect controller A port e11b to port NSM B port e0b.
+
image:../media/drw_a5x_1shelf_cabling_a_ieops-xxxx.svg[Controller A e11a and e11b to a single NS224 shelf]

. Connect controller B port e11a to NSM B port e0a.
. Connect controller B port e11b to NSM A port e0b.
+
image:../media/drw_a5x_1shelf_cabling_b_ieops-xxxx.svg[Controller B e11a and e11b to a single NS224 shelf]

--

====

.What's next?
After you've cabled the hardware, you link:install-power-hardware.html[power on the controllers].